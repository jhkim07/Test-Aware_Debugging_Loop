# Test-Aware Debugging Loop (SWE-bench + pytest) â€” MVP Scaffold

This repository is a **minimal, runnable scaffold** for a "Test-Aware Debugging Loop Benchmark" on **SWE-bench** instances:
- Each iteration **must** produce a **test diff** (intermediate artifact) and a **code patch diff**.
- The runner evaluates:
  - whether the generated tests reproduce the bug (**BRS**)
  - whether the combined patch passes the repository tests (public)
  - optional hidden/holdout evaluation hooks (stubbed for MVP)

It is designed for **Ubuntu 22.04 + conda + Docker**.

## 0) Prerequisites
- Docker installed and usable by your user (Linux post-install steps).
- Conda environment with Python 3.11+ recommended.
- OpenAI API key exported as `OPENAI_API_KEY`.

## 1) Install
```bash
conda create -n ta-swebench python=3.11 -y
conda activate ta-swebench
pip install -U pip
pip install swebench pyyaml openai rich
```

SWE-bench harness uses Docker (required). See SWE-bench evaluation guide. îˆ€citeîˆ‚turn0search5îˆ

## 2) Configure instances
Edit `configs/mvp.yaml` and add **10â€“50** SWE-bench instance IDs (Lite/Verified/etc).

Example instance IDs:
- `astropy__astropy-14539`
- `sympy__sympy-20590`

## 3) Run (single or batch)

### ì¼ë°˜ ì‹¤í–‰ (Cursor ì°½ì„ ë‹«ìœ¼ë©´ ì¢…ë£Œë  ìˆ˜ ìˆìŒ)
```bash
python scripts/run_mvp.py --config configs/mvp.yaml --run-id mvp-001 --max-workers 2
```

### ì¥ì‹œê°„ ì‹¤í–‰ (Cursor ì°½ì„ ë‹«ì•„ë„ ê³„ì† ì‹¤í–‰ë¨) â­ ê¶Œì¥
```bash
./scripts/run_mvp_nohup.sh configs/mvp.yaml mvp-001 1
```

### ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ í™•ì¸
```bash
# ì‹¤í—˜ ìƒíƒœ í™•ì¸ (ë‹¤ìŒ Cursor ì„¸ì…˜ì—ì„œ ì‹¤í–‰)
./scripts/check_experiment.sh

# ë˜ëŠ” ë¡œê·¸ í™•ì¸
tail -f logs/<run-id>.log
```

**ğŸ’¡ íŒ**: ë‹¤ìŒ Cursor ì„¸ì…˜ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ì„ í™•ì¸í•˜ë ¤ë©´ í”„ë¡œì íŠ¸ë¥¼ ì—´ê³  `./scripts/check_experiment.sh`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.

The runner will create:
- `outputs/<run-id>/<instance-id>/` with:
  - `run.jsonl` (iteration logs)
  - `predictions.jsonl` (latest patch for swebench harness)
  - `final_patch.diff`, `final_tests.diff`
  - `metrics.json`

## Notes on "public/hidden" split (MVP)
This scaffold implements the **protocol** and produces the required artifacts, and includes a **hook** for hidden evaluation.

To fully implement a public/hidden split for pytest you typically need either:
1) a controlled split of test files and a way to run hidden tests separately, or
2) a separate harness run using a custom test command.

The `bench_agent/runner/hidden_eval.py` file is a **stub** where you can integrate your preferred approach.

## Policy guards (enforced heuristically)
- no `pytest.skip` / `xfail` additions
- no obvious network calls in tests (`requests`, `urllib`, `socket`)
- file I/O is restricted (heuristics; adjust for your repo needs)

## References
- `python -m swebench.harness.run_evaluation ...` CLI usage. îˆ€citeîˆ‚turn0search1îˆ‚turn0search5îˆ
